<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vidhisha Balachandran on Vidhisha Balachandran</title>
    <link>https://vidhishanair.github.io/</link>
    <description>Recent content in Vidhisha Balachandran on Vidhisha Balachandran</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Vidhisha Balachandran 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Experience</title>
      <link>https://vidhishanair.github.io/experience/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vidhishanair.github.io/experience/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluating and Mitigating Factual Inconsistencies in Language Generation</title>
      <link>https://vidhishanair.github.io/talk/dlct/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/talk/dlct/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Actionable Directions for Reporting and Mitigating Language Model Harms</title>
      <link>https://vidhishanair.github.io/talk/georgetown/</link>
      <pubDate>Tue, 20 Jun 2023 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/talk/georgetown/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge</title>
      <link>https://vidhishanair.github.io/project/cook/</link>
      <pubDate>Thu, 18 May 2023 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/cook/</guid>
      <description>&lt;p&gt;Large language models (LLMs) are increasingly adopted for knowledge-intensive tasks and contexts. Existing approaches improve the knowledge capabilities of general-purpose LLMs through retrieval or generated knowledge prompting, but they fall short of reflecting two key properties of knowledge-rich models: knowledge should be modular, ever-growing, sourced from diverse domains; knowledge acquisition and production should be a collaborative process, where diverse stakeholders contribute new information. To this end, we propose CooK, a novel framework to empower general-purpose large language models with modular and collaboratively sourced knowledge. We first introduce specialized language models, autoregressive models trained on corpora from a wide range of domains and sources. These specialized LMs serve as parametric knowledge repositories that are later prompted to generate background knowledge for general-purpose LLMs. We then propose three knowledge filters to dynamically select and retain information in generated documents by controlling for relevance, brevity, and factuality. Finally, we propose bottom-up and top-down knowledge integration approaches to augment general-purpose LLMs with the curated (relevant, factual) knowledge from community-driven specialized LMs that enable multi-domain knowledge synthesis and on-demand knowledge requests. Through extensive experiments, we demonstrate that CooK achieves state-of-the-art performance on six benchmark datasets. Our results highlight the potential of enriching general-purpose LLMs with evolving and modular knowledge &amp;ndash; relevant knowledge that can be continuously updated through the collective efforts of the research community.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge</title>
      <link>https://vidhishanair.github.io/project/factkb/</link>
      <pubDate>Thu, 18 May 2023 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/factkb/</guid>
      <description>&lt;p&gt;Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge</title>
      <link>https://vidhishanair.github.io/publication/factkb/</link>
      <pubDate>Thu, 18 May 2023 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/factkb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generalizable Factual Error Correction of Model Generated Summaries</title>
      <link>https://vidhishanair.github.io/talk/semafor/</link>
      <pubDate>Thu, 18 May 2023 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/talk/semafor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Knowledge Card: Filling LLMs&#39; Knowledge Gaps with Plug-in Specialized Language Models</title>
      <link>https://vidhishanair.github.io/publication/cook/</link>
      <pubDate>Thu, 18 May 2023 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/cook/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey</title>
      <link>https://vidhishanair.github.io/project/harms_survey/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/project/harms_survey/</guid>
      <description>&lt;p&gt;Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have identified potential causes of these harms and called for their mitigation via development of safer and fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works&amp;rsquo; taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners with explanations of motivations behind different mitigation strategies, their limitations, and open problems for future research.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey</title>
      <link>https://vidhishanair.github.io/publication/harms_survey/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/harms_survey/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unsupervised Keyphrase Extraction via Interpretable Neural Networks</title>
      <link>https://vidhishanair.github.io/project/inspect/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/project/inspect/</guid>
      <description>&lt;p&gt;Keyphrase extraction aims at automatically extracting a list of ``important&amp;rdquo; phrases representing the key concepts in a document.
    Prior approaches for unsupervised keyphrase extraction resort to heuristic notions of phrase importance via embedding similarities or graph centrality, requiring extensive domain expertise to develop them.
    Our work presents an alternative operational definition: phrases that are most useful for predicting the topic of a text are keyphrases.
    To this end, we propose INSPECT&amp;mdash;a self-explaining neural framework for identifying influential keyphrases by measuring the predictive impact of input phrases on the downstream task of topic classification.
    We show that this novel approach not only alleviates the need for ad-hoc heuristics but also achieves state-of-the-art results in unsupervised keyphrase extraction in 3 out of 4 diverse datasets across two domains: scientific publications and news articles. Ultimately, our study suggests a new usage of interpretable neural networks as an intrinsic component in NLP systems, and not only as a tool for explaining model predictions to humans.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Keyphrase Extraction via Interpretable Neural Networks</title>
      <link>https://vidhishanair.github.io/publication/inspect/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/inspect/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling</title>
      <link>https://vidhishanair.github.io/project/factedit/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/factedit/</guid>
      <description>&lt;p&gt;Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets &amp;ndash; CNN/DM and XSum &amp;ndash; we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model &amp;ndash; FactEdit &amp;ndash; improves factuality scores by over ~11 points on CNN/DM and over ~31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling</title>
      <link>https://vidhishanair.github.io/publication/factedit/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/factedit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LEXplain: Improving Model Explanations via Lexicon Supervision</title>
      <link>https://vidhishanair.github.io/publication/lexplain/</link>
      <pubDate>Sat, 23 Apr 2022 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/lexplain/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
