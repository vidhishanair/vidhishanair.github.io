<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vidhisha Balachandran on Vidhisha Balachandran</title>
    <link>https://vidhishanair.github.io/</link>
    <description>Recent content in Vidhisha Balachandran on Vidhisha Balachandran</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Vidhisha Balachandran 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Experience</title>
      <link>https://vidhishanair.github.io/experience/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vidhishanair.github.io/experience/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Simple and Efficient ways to Improve REALM</title>
      <link>https://vidhishanair.github.io/publication/realm/</link>
      <pubDate>Sun, 18 Apr 2021 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/realm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers</title>
      <link>https://vidhishanair.github.io/project/selfexplain/</link>
      <pubDate>Tue, 23 Mar 2021 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/project/selfexplain/</guid>
      <description>&lt;p&gt;Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers</title>
      <link>https://vidhishanair.github.io/publication/selfexplain/</link>
      <pubDate>Tue, 23 Mar 2021 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/selfexplain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues</title>
      <link>https://vidhishanair.github.io/publication/dialograph/</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/dialograph/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differentiable Reasoning over a Virtual Knowledge Base</title>
      <link>https://vidhishanair.github.io/project/vkb/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 -0500</pubDate>
      
      <guid>https://vidhishanair.github.io/project/vkb/</guid>
      <description>&lt;p&gt;We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a virtual KB, softly following paths of relations between mentions of entities in the corpus. At each step the operation uses a combination of sparse-matrix TFIDF indices and maximum inner product search (MIPS) on a special index of contextual representations. This module is differentiable, so the full system can be trained completely end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the index mention encoder by generating hard negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by 70%. DrKIT is also very efficient, processing upto 10x more queries per second than existing state-of-the-art QA systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Closed Domain Entity Recognition and Fraud Detection</title>
      <link>https://vidhishanair.github.io/project/entity/</link>
      <pubDate>Sat, 02 Mar 2019 00:00:00 -0500</pubDate>
      
      <guid>https://vidhishanair.github.io/project/entity/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differentiable Reasoning over a Virtual Knowledge Base</title>
      <link>https://vidhishanair.github.io/publication/diffindex/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/diffindex/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning to Define Terms in the Software Domain</title>
      <link>https://vidhishanair.github.io/publication/defgen/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/defgen/</guid>
      <description></description>
    </item>
    
    <item>
      <title>StructSum: Summarization via Structured Representations</title>
      <link>https://vidhishanair.github.io/publication/structsum/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/structsum/</guid>
      <description></description>
    </item>
    
    <item>
      <title>StructSum: Summarization via Structured Representations</title>
      <link>https://vidhishanair.github.io/project/structsum/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/project/structsum/</guid>
      <description>&lt;p&gt;Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>https://vidhishanair.github.io/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/tutorial/example/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Table to Text Generation</title>
      <link>https://vidhishanair.github.io/project/tabletotext/</link>
      <pubDate>Sun, 27 May 2018 00:00:00 -0400</pubDate>
      
      <guid>https://vidhishanair.github.io/project/tabletotext/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Definition Generation</title>
      <link>https://vidhishanair.github.io/project/defgen/</link>
      <pubDate>Mon, 04 Dec 2017 00:00:00 -0500</pubDate>
      
      <guid>https://vidhishanair.github.io/project/defgen/</guid>
      <description>&lt;p&gt;One way to test a personâ€™s knowledge of a domain is to ask them to define domain-specific terms. Here, we investigate the task of automatically generating definitions of technical terms by reading text from the technical domain. Specifically, we learn definitions of software entities from a large corpus built from the user forum Stack Overflow. To model definitions, we train a language model and incorporate additional domain-specific information like word-word co-occurrence, and ontological category information. Our approach improves previous baselines by 2 BLEU points for the definition generation task. Our experiments also show the additional challenges associated with the task and the short-comings of language-model based architectures for definition generation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://vidhishanair.github.io/talk/example/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>https://vidhishanair.github.io/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;em&gt;Slides&lt;/em&gt; feature and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
