<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vidhisha Balachandran on Vidhisha Balachandran</title>
    <link>https://vidhishanair.github.io/</link>
    <description>Recent content in Vidhisha Balachandran on Vidhisha Balachandran</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Vidhisha Balachandran 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Experience</title>
      <link>https://vidhishanair.github.io/experience/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vidhishanair.github.io/experience/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey</title>
      <link>https://vidhishanair.github.io/project/harms_survey/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/project/harms_survey/</guid>
      <description>&lt;p&gt;Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have identified potential causes of these harms and called for their mitigation via development of safer and fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works&amp;rsquo; taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners with explanations of motivations behind different mitigation strategies, their limitations, and open problems for future research.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey</title>
      <link>https://vidhishanair.github.io/publication/harms_survey/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/harms_survey/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unsupervised Keyphrase Extraction via Interpretable Neural Networks</title>
      <link>https://vidhishanair.github.io/project/inspect/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/project/inspect/</guid>
      <description>&lt;p&gt;Keyphrase extraction aims at automatically extracting a list of ``important&amp;rdquo; phrases representing the key concepts in a document.
    Prior approaches for unsupervised keyphrase extraction resort to heuristic notions of phrase importance via embedding similarities or graph centrality, requiring extensive domain expertise to develop them.
    Our work presents an alternative operational definition: phrases that are most useful for predicting the topic of a text are keyphrases.
    To this end, we propose INSPECT&amp;mdash;a self-explaining neural framework for identifying influential keyphrases by measuring the predictive impact of input phrases on the downstream task of topic classification.
    We show that this novel approach not only alleviates the need for ad-hoc heuristics but also achieves state-of-the-art results in unsupervised keyphrase extraction in 3 out of 4 diverse datasets across two domains: scientific publications and news articles. Ultimately, our study suggests a new usage of interpretable neural networks as an intrinsic component in NLP systems, and not only as a tool for explaining model predictions to humans.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Keyphrase Extraction via Interpretable Neural Networks</title>
      <link>https://vidhishanair.github.io/publication/inspect/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/inspect/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling</title>
      <link>https://vidhishanair.github.io/publication/factedit/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/factedit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self Attention and Transformers for Undergrad NLP</title>
      <link>https://vidhishanair.github.io/talk/transf/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/talk/transf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Investigating the Effect of Background Knowledge on Natural Questions</title>
      <link>https://vidhishanair.github.io/project/fatnq/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/fatnq/</guid>
      <description>&lt;p&gt;Existing work shows the benefits of integrating KBs with textual evidence for QA only on questions that are answerable by KBs alone (Sun et al., 2019). In contrast, real world QA systems often have to deal with questions that might not be directly answerable by KBs. Here, we investigate the effect of integrating background knowledge from KBs for the Natural Questions (NQ) task. We create a subset of the NQ data, Factual Questions (FQ), where the questions have evidence in the KB in the form of paths that link question entities to answer entities but still must be answered using text, to facilitate further research into KB integration methods. We propose and analyze a simple, model-agnostic approach for incorporating KB paths into text-based QA systems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simple and Efficient ways to Improve REALM</title>
      <link>https://vidhishanair.github.io/talk/realm/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/talk/realm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>StructSum: Summarization via Structured Representations</title>
      <link>https://vidhishanair.github.io/publication/structsum/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/structsum/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Transparency and Reliability of Automatic Summarization</title>
      <link>https://vidhishanair.github.io/talk/transp_summ/</link>
      <pubDate>Fri, 30 Apr 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/talk/transp_summ/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics</title>
      <link>https://vidhishanair.github.io/project/frank/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/frank/</guid>
      <description>&lt;p&gt;Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights into the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations, we identify the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgment as well as their specific strengths and weaknesses.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics</title>
      <link>https://vidhishanair.github.io/publication/frank/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/frank/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers</title>
      <link>https://vidhishanair.github.io/publication/selfexplain/</link>
      <pubDate>Fri, 23 Apr 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/publication/selfexplain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Simple and Efficient ways to Improve REALM</title>
      <link>https://vidhishanair.github.io/project/realm/</link>
      <pubDate>Sun, 18 Apr 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/realm/</guid>
      <description>&lt;p&gt;Dense retrieval has been shown to be effective for retrieving relevant documents for Open Domain QA, surpassing popular sparse retrieval methods like BM25. REALM (Guu et al., 2020) is an end-to-end dense retrieval system that relies on MLM based pretraining for improved downstream QA efficiency across multiple datasets. We study the finetuning of REALM on various QA tasks and explore the limits of various hyperparameter and supervision choices. We find that REALM was significantly undertrained when finetuning and simple improvements in the training, supervision, and inference setups can significantly benefit QA results and exceed the performance of other models published post it. Our best model, REALM++, incorporates all the best working findings and achieves significant QA accuracy improvements over baselines (~5.5% absolute accuracy) without any model design changes. Additionally, REALM++ matches the performance of large Open Domain QA models which have 3x more parameters demonstrating the efficiency of the setup.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
