<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Factuality on Vidhisha Balachandran</title>
    <link>https://vidhishanair.github.io/tags/factuality/</link>
    <description>Recent content in Factuality on Vidhisha Balachandran</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Vidhisha Balachandran 2018</copyright>
    <lastBuildDate>Thu, 18 May 2023 00:00:00 -0700</lastBuildDate>
    
	<atom:link href="https://vidhishanair.github.io/tags/factuality/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge</title>
      <link>https://vidhishanair.github.io/project/cook/</link>
      <pubDate>Thu, 18 May 2023 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/cook/</guid>
      <description>Large language models (LLMs) are increasingly adopted for knowledge-intensive tasks and contexts. Existing approaches improve the knowledge capabilities of general-purpose LLMs through retrieval or generated knowledge prompting, but they fall short of reflecting two key properties of knowledge-rich models: knowledge should be modular, ever-growing, sourced from diverse domains; knowledge acquisition and production should be a collaborative process, where diverse stakeholders contribute new information. To this end, we propose CooK, a novel framework to empower general-purpose large language models with modular and collaboratively sourced knowledge.</description>
    </item>
    
    <item>
      <title>FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge</title>
      <link>https://vidhishanair.github.io/project/factkb/</link>
      <pubDate>Thu, 18 May 2023 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/factkb/</guid>
      <description>Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases.</description>
    </item>
    
    <item>
      <title>Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey</title>
      <link>https://vidhishanair.github.io/project/harms_survey/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 -0800</pubDate>
      
      <guid>https://vidhishanair.github.io/project/harms_survey/</guid>
      <description>Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have identified potential causes of these harms and called for their mitigation via development of safer and fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models.</description>
    </item>
    
    <item>
      <title>Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling</title>
      <link>https://vidhishanair.github.io/project/factedit/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/factedit/</guid>
      <description>Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models.</description>
    </item>
    
    <item>
      <title>Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics</title>
      <link>https://vidhishanair.github.io/project/frank/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/frank/</guid>
      <description>Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights into the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets.</description>
    </item>
    
  </channel>
</rss>