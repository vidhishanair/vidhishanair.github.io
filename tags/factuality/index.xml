<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Factuality on Vidhisha Balachandran</title>
    <link>https://vidhishanair.github.io/tags/factuality/</link>
    <description>Recent content in Factuality on Vidhisha Balachandran</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Vidhisha Balachandran 2018</copyright>
    <lastBuildDate>Mon, 10 Oct 2022 00:00:00 -0700</lastBuildDate>
    
	<atom:link href="https://vidhishanair.github.io/tags/factuality/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling</title>
      <link>https://vidhishanair.github.io/project/factedit/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/factedit/</guid>
      <description>Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models.</description>
    </item>
    
    <item>
      <title>Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics</title>
      <link>https://vidhishanair.github.io/project/frank/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/frank/</guid>
      <description>Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights into the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets.</description>
    </item>
    
  </channel>
</rss>