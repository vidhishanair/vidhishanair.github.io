<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Interpretability by Design on Vidhisha Balachandran</title>
    <link>https://vidhishanair.github.io/tags/interpretability-by-design/</link>
    <description>Recent content in Interpretability by Design on Vidhisha Balachandran</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Vidhisha Balachandran 2018</copyright>
    <lastBuildDate>Sat, 10 Jul 2021 00:00:00 -0700</lastBuildDate>
    
	<atom:link href="https://vidhishanair.github.io/tags/interpretability-by-design/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>LEXplain: Improving Model Explanations via Lexicon Supervision</title>
      <link>https://vidhishanair.github.io/project/lexplain/</link>
      <pubDate>Sat, 10 Jul 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/lexplain/</guid>
      <description>Model explanations that shed light on the model{&amp;lsquo;}s predictions are becoming a desired additional output of NLP models, alongside their predictions. Challenges in creating these explanations include making them trustworthy and faithful to the model{&amp;rsquo;}s predictions. In this work, we propose a novel framework for guiding model explanations by supervising them explicitly. To this end, our method, LEXplain, uses task-related lexicons to directly supervise model explanations. This approach consistently improves the model{&amp;lsquo;}s explanations without sacrificing performance on the task, as we demonstrate on sentiment analysis and toxicity detection.</description>
    </item>
    
    <item>
      <title>SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers</title>
      <link>https://vidhishanair.github.io/project/selfexplain/</link>
      <pubDate>Tue, 23 Mar 2021 00:00:00 -0700</pubDate>
      
      <guid>https://vidhishanair.github.io/project/selfexplain/</guid>
      <description>We introduce SelfExplain, a novel self-explaining framework that explains a text classifier&amp;rsquo;s predictions using phrase-based concepts. SelfExplain augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five text-classification datasets show that SelfExplain facilitates interpretability without sacrificing performance.</description>
    </item>
    
  </channel>
</rss>