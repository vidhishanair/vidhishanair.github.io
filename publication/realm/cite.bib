@inproceedings{balachandran-etal-2021-simple,
    title = "Simple and Efficient ways to Improve {REALM}",
    author = "Balachandran, Vidhisha  and
      Vaswani, Ashish  and
      Tsvetkov, Yulia  and
      Parmar, Niki",
    booktitle = "Proceedings of the 3rd Workshop on Machine Reading for Question Answering",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.mrqa-1.16",
    doi = "10.18653/v1/2021.mrqa-1.16",
    pages = "158--164",
    abstract = "Dense retrieval has been shown to be effective for Open Domain Question Answering, surpassing sparse retrieval methods like BM25. One such model, REALM, (Guu et al., 2020) is an end-to-end dense retrieval system that uses MLM based pretraining for improved downstream QA performance. However, the current REALM setup uses limited resources and is not comparable in scale to more recent systems, contributing to its lower performance. Additionally, it relies on noisy supervision for retrieval during fine-tuning. We propose REALM++, where we improve upon the training and inference setups and introduce better supervision signal for improving performance, without any architectural changes. REALM++ achieves {\textasciitilde}5.5{\%} absolute accuracy gains over the baseline while being faster to train. It also matches the performance of large models which have 3x more parameters demonstrating the efficiency of our setup.",
}
